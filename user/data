data
                                    * * *

The notes plug-in comes with self hosting documentation. To jump to these notes
position your cursor on the highlighted name and press ‘gf’ in normal mode:

 • Note taking syntax
 • Note taking commands

                                     * * *

 
# Data Structures

## Chapter 1:

How Java Works:
   Java Program → Compiler → Bytecode -> Java virtual machine → Result

   • An *OBJECT* is a repository of data.
   • *TYPED DATA* always declare before use.
   • *PRIMITIVE TYPES*: int, char, bool, float, etc.
   • A *CLASS* is a template for creating objects.

   More on Data Types:
      • *BOOLEAN*: 1 or 0.
      • *CHAR*: character like ‘s’.
      • *BYTE*: 8-bit integer.
      • *SHORT*: 16-bit integer.
      • *INT*: 32-bit integer.
      • *LONG*: 64-bit integer.
      • *FLOAT*: 32-bit floating-point number.
      • *DOUBLE*: 32-bit floating-point number.
      • *REFERENCE VARIABLE*: a variable whose type is a class.

Access Control Modifiers: 
   • *PRIVATE*: only visible within class.
   • *NO MODIFIER*: visible within package. (default)
   • *PROTECTED*: visible to the package and all subclasses.
   • *PUBLIC*: visible to everything.
   
How to Organize a Class:
   1) Constants
   2) Instance variables
   3) Constructors
   4) Methods
   
Methods:
   • An *ACCESSOR METHOD* returns a value but takes no input.
   • An *UPDATE METHOD* changes a value but returns nothing (void).

      {{{java Person S;
      S = new Person("Sam");
      String name = S.getName(); //accessor method getName() returns string "Sam"
      }}}
Strings:
   • Java strings are *immutable objects*, so they can’t actually be modified, they just get remade.

      {{{java String s1; //declare variable
      s1 = new String(); //assign s1 to be an empty string object
      String s1 = new String(); // s2 new empty string
      s1 = "hello";
      s2 = s1; //s2 now also points to "hello"
      }}}
         > Both s1 and s2 point to the same “hello” string. They aren’t actually separate entities.

      {{{java s2 = new String(s1); //this will actually create a separate copy of the s1 string "hello" for s2.
      }}}
   • If you edit a string in java, you are copying the old one, making the change, and then reassigning it to the variable. 

      {{{java s1 = "hello";
      s1 += '!'; // copies "hello" and adds '!' to it. s1 now = "hello!"
      }}}
Arrays:
   • A numbered list of variables.
   • An array variable is a *reference variable*.
   
      {{{java int[] array = new int[3]; // array = [][][];
      int len = array.length; // .length method returns length of an array
      }}}
   • Arrays are *immutable objects*.
      
   Common ways to work with arrays:
      • *java.util.Arrays* methods for arrays support some of the following:
      
      {{{java Arrays.equals(a,b); // same as a == b. Returns a bool.
      Arrays.copyOf(A,n); //makes a copy of A into n.
      Arrays.toString(A); //returns a string of the variable in array A.
      Arrays.sort(A); //sorts A in ascending order.
      }}}
Linked Lists:
   • Because arrays are *immutable*; you can’t add new items to them without copying the entire array.
   • *Linked Lists* allow you to manipulate the lists in *constant time*.

      {{{java public class Node{
         int item; // list item (can be any data type).
         Node next; //recursive definition: points to the next Node!
      }
      Node A = new.Node();
      Node B = new.Node();
      Node C = new.Node(); //create 3 items for the list
      A.item = 1;
      B.item = 2;
      C.item = 3; //populate the list
      A.next = B;
      B.next = C;
      C.next = null;
      }}}
Complexity of Algorithms:
   • *TIME COMPLEXITY* is abstracted to the number of steps or basic operations performed in the worst case during a computation.
   • A *DATA STRUCTURE* is a systematic way of organizing and accessing data.
   • An *ALGORITHM* is a step-by-step procedure for performing some task in a finite amount of time.
   • A *PRIMITIVE OPERATION* is a constant time, like calling a method, etc.
   • To capture the order of growth for an algorithm’s running time, we use a function of the number of primitive operations.

      *f(n)* = # of primitive operations that are performed as a function of input size *n*. 
      
   Constant Function:
      _f(n) = c_

      • For some fixed constant *c*
         ▸ It Doesn’t matter what *n* is. The algorithm takes a constant amount of time to complete regardless.

   Logarithm Function:
      _f(n) = log b (n)_

         >if and only if
         b^x = n

         >What the fuck does log even do?
         > *Log b (n)*
         > Takes *n* and divides it by *b* until getting a number <= 1.

      • O(log n) algorithms are great because they reduce the data being used by about half after each iteration.
      • Since computers are binary, most of the time it will be log base b = 2 {log 2 (n) == log(n)}
      • Using rule 4 below: log2 (n) = log(n) / log(2) 
         > This is how to convert from log10 to log2 on a calculator.

      Logarithm Computation:
         1. logb(ac) = logb a+logb c
         2. logb(a/c) = logb a−logb c
         3. logb(ac) = clogb a
         4. logb a = logd a/logd b
         5. blogd a = alogd b

   Linear Function:
      _f(n) = n_

      • Time increases linearly as size *n* increases
         ▸ For example: searching an array.

   N-Log-N Function:
      _f(n) = n log n_
      
      • The fastest sorting algorithms for sorting *N* items are all in *N-Log-N* time.
      • This can be interpreted mathematically as: 
         Comparisons = log n!
         Comparisons = log(n) + log(n-1) + log(n-2) ... + log(1)
         Comparisons = nlog(n)

   Quadratic Function:
      _f(n) = n^2_

      • You will see algos in quadratic time in cases such as *nested loops*.
         ▸ This is because the first loop does *n* computations, and inner loop also does *n* computations.
      • O(n^2) algorithms are typically avoided if possible because they become really inefficient the higher *n* gets.

   Polynomial Function:
      _f(n) = a0 + a1(n) + a2(n^2) + a3(n^3) ... + ad(n^d)_

         > {a0,a1,...ad} are constants called the *coefficients* of the polynomial.
         > for ad, integer *d* is the *degree* of the polynomial.

         How to find the Polynomial of a method:
            • The cost of each action within a function is each *comparison* or *assignment* that occurs.
            • Essentially, you look for + - =. Any of that kind of shit is a computation.

               {{{java listSum(A,n){
                  int  total = 0;                    // cost: 1 unit (c1)    # of times: 1
                  for (int i = 0; i < n; i++){       //       2 (c2)                     n+1    <- that +1 is for the exit case of the loop. It has to run one extra time to fail.
                     total = total + A[i];           //       2 (c3)                     n
                  }
                  return total;                      //       1 (c4)                     1
               }                                     // Add all these terms together:     1 + 2(n+1) + 2(n) + 1   ->  = 4n + 4
            }}}
                  → From *4n + 4* 
                  T(n) = cn + c’  (c is a constant)
                  → Where:
                  c = c2 + c3
                  c’ = c1 + c2 + c4

   Exponential Function:
      _f(n) = b^n_

      • most common base(*b*) in computing is 2.
 
Asymptotic Analysis:
   • Normally we are just interested in getting the big picture of the growth rate of a running time for size *n*.
   • It is often enough just to know that the algorithm grows proportionally to *n*.
   • We can estimate the number of primitive operations executed up to a constant factor.
   • For example; T(n) = 17n^2 + 6n + 7
      ◦ As n → infinity
      ◦ 6n + 7 become irrelevant as n gets bigger and bigger so we just ignore those terms.

   Big O Notation:
      • _WORST CASE TIME_
      Consider f(n) and g(n):
         f(n) is O(g(n)) *if* 
            1) there is a real constant *c > 0*.
            2) there is an integer constant *n >= n0*.

            “f(n) is big-O of g(n).”

            For example;
               f(n) = 8n + 5 is O(n).
                  > we need to find a *c* > 0 and *n* >= 1
                  > such that *8n + 5 <= cn* for every integer *n* >= n0.
               c = 9 and n0 = 5
     
      • This is another way of saying that *f(n)* is less than or equal to a function *g(n)* up to a constant factor asymptotically.
      • Big O notation allows us to ignore constant factors and lower order terms to focus on the main components of a function that affect its growth.
      
      • We can characterize the growth rate of any *polynomial function* of degree *d*:
         ▸ f(n) = a0 + a1(n) + a2(n^2) ... + ad(n^d)
         ▸ and ad > 0, then f(n) is *O(n^d)*.
         ▸ 
   
   Big Omega Notation: (horseshoe symbol)
      • _BEST CASE TIME_
      • Show that an algorithm takes *at least* a certain amount of time.
         ▸ As opposed to O(n) notation that shows *at most* amount of time.

      General Rules for Time Complexity Analysis:
         • Interested in massive input size.
         • Worst case scenario in case of conditional statements (look at *else* statements).
            T(n) = n^3 + 3n^2 + 4n + 2
               1) drop lower order terms
               2) drop constant multiplier
               = n^3  (n → infinity) 
               will not grow any faster than:
               cn^3 == O(n^3)
                     
         • Running Time: 
            1) int a;
            2) a = 5;
            3) a ++;   “3 simple statements” _O(1)_
h           4) for (i = 0; i < n; i++){ simple statements }; _O(n)_
            T(n) = O(1) + O(n) “take highest order and then ignore the rest”
            T(n) = O(n)


Recursion:
   • When a method calls itself.

   Example; 
      Factorial(n) = n * (n-1) * (n-2) * (n-3) ... * 2 * 1
      Base case → Factorial(1) = 1
      Recursive step → Factorial(n) = n * Factorial(n-1)

         {{{java public static long factorial(int n){
            if (n == 1) return 1;
            return n * factorial(n-1);
         }
      }}}
      • You need a *base case* for all recursive methods because otherwise it would keep calling itself forever.
         ▸ In the example above, the method will stop calling itself once it gets 1 as a value because you can’t take the factorial of anything below 1.

#Chapter 14: Graphs

What is a graph:
   • A set of nodes(vertices) and edges

   • *Digraph* - directed edges.

   • *Path* - a sequence of edges that starts at a vertex and ends at a vertex such that each edge is incident to it’s prev/next vertex’s. 
   • *Cycle* - a path that starts & ends at the same vertex. Includes at least 1 edge.


Formulas: 

   1) graph G with m edges and vertex set V.

      sum(v in V) degree(v) = 2m
   


#Chapter 11: Trees

Tree Traversals:

A B C D E F G H I

_DEPTH-FIRST_ - use a *stack*.
   • *Pre—Order* O(n)
      ◦ 1. visit node
      ◦ 2. left subtree
      ◦ 3. right subtree
   • *In-Order* 
      ◦ 1. left subtree
      ◦ 3. visit node
      ◦ 2. right subtree
   • *Post-Order* 
      ◦ 1. left subtree
      ◦ 2. right subtree
      ◦ 3. visit node

_BREADTH-FIRST_ - use a *queue*.

   • *Level-Order* O(n)
      ◦ node = head
         ▸ 1. add node to queue
         ▸ 2. add it’s children to queue
         ▸ 3. dequeue node
            ▹ repeat 1,2,3 with dequeued node.
            ▹ recurse until queue is empty.

Binary Search Tree:
   • Every right-child is less than it’s parent. Every left-child is greater than it’s parent.

Red-Black Tree:

   • Every node is either *red* or *black*.    

   h <= 2 log(n+1)

      ◦ Search, insert, delete in *O(log n)*
      ◦ Better than AVL Tree because:
         ▸ each insertion needs *at most* 1 rotation.
         ▸ each deletion needs at most 2 rotations.
      ◦ Therefore, RB-Trees are better when you need to do frequent insertions & deletions.

   _Properties_
      1. The *root* node is always *black*. (*Root* *Property*)
      2. Every *null* node is *black*. (*External* *Property*)
      3. Every *red* node has two *black* children. (*Red Property*)
      4. Every path from node *x* down to a leaf node has the same number of *black* nodes. (*Depth Property*)

Minimum Spanning Tree: 

• In a weighted graph 
• The weight of a Spanning Tree == sum of all edge weights.
• Of all the spanning trees, the one with the smallest weight is the minumum spanning tree.
• Uses two algorithm’s to find:

   Kruskal’s Algorithm:
      • for graph G:
      1) Create an empty graph T with same vertices as G but no edges.
      2) Make priority queue with G’s edges. 
         ▸ Prioritize lowest weight.
      3) While number of edges in T < n-1
         ▸ Remove edge(u,v) from Q
         ▸ If u and v are not connected, then add (u,v) to T.

   Prim’s Algorithm:
      1) Choose an arbitrary vertex.
      2) Grow the tree by one edge at a time.
         ▸ Find the minimum weighted outgoing edge (Priority Queue)
         ▸ Add that edge to tree.
         ▸ Repeat until no outgoing edge is left.
         
      ◦ An edge is outgoing if it connects a node to another node not yet in the tree.

Computing Shortest Path:
• Given a weighted graph G, what is the shortest path from node u to v?

   Dijkstra’s Algorithm:
      ◦ shortest path from *one* node to every other node.

      1) Pick a node *a*.
      2) For every other node in the graph:
         ▸ Label with ( _ , -1)   “(node reached from , edge distance)
      3) Starting from *a*:
         ▸ Travel to all adjacent nodes and label them with the node you’re reaching them from (a) and the edge distance.
      4) Choose the edge with the smallest distance, for example node *b*
         ▸ This edge is now part of the MST.
      5) Now repeat the process for any node neighboring either *a* or *b*.
         ▸ Keep adding the minumum edge distance to the MST
































